{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-06-30.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP5hKKLU/PtlWCYFXxhdeSS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Construct a probabilistic mapping between one set of integers and another, randomly generate the mapping and then create a function which performs the mapping. Then learn the mapping."],"metadata":{"id":"sv_zwlBsrRTp"}},{"cell_type":"code","source":["%pip install optax"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-yEb5sjz5fBE","executionInfo":{"status":"ok","timestamp":1656599378264,"user_tz":240,"elapsed":3237,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}},"outputId":"e24b4fd0-a5c6-4ce8-c3ae-86731437a2a9"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.8)\n","Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.7+cuda11.cudnn805)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from optax) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from optax) (4.1.1)\n","Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from optax) (1.1.0)\n","Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.3)\n","Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.7)\n","Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (1.4.1)\n","Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n"]}]},{"cell_type":"code","execution_count":126,"metadata":{"id":"8jA54f7Aq-xy","executionInfo":{"status":"ok","timestamp":1656600092912,"user_tz":240,"elapsed":119,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"outputs":[],"source":["from collections import namedtuple\n","from functools import partial\n","import jax\n","import jax.numpy as jnp\n","from jax import random\n","from jax import jit, vmap, grad\n","from jax import nn\n","import optax\n","import numpy as np\n","rng = random.PRNGKey(42)"]},{"cell_type":"code","source":["# how would we model a probability distribution over a set of choices?\n","# start with a set of unnormalized values over the choices then turn that into\n","# a probability distribution and sample from it"],"metadata":{"id":"l8oQHrqhrK4d","executionInfo":{"status":"ok","timestamp":1656599378265,"user_tz":240,"elapsed":4,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["num_choices = 16\n","rng, r = random.split(rng)\n","logits = random.normal(r, shape=(num_choices,))\n","logits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHEux3YNsJYk","executionInfo":{"status":"ok","timestamp":1656599379225,"user_tz":240,"elapsed":117,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}},"outputId":"33d41ac0-59f6-49a4-d005-03b10ee5649b"},"execution_count":107,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray([ 0.7679137 ,  0.46966743, -1.4884446 , -1.155719  ,\n","             -1.2574353 , -0.66447204, -1.3314192 ,  0.6470733 ,\n","             -0.55124223, -0.3213504 ,  1.1168208 , -0.4216216 ,\n","              0.32398054,  1.3500887 , -0.22909231,  0.24462827],            dtype=float32)"]},"metadata":{},"execution_count":107}]},{"cell_type":"code","source":["# now sample from the logits\n","num_samples = 2**14\n","rng, r = random.split(rng)\n","samples = random.categorical(r, logits, shape=(num_samples,))\n","samples.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsRq_wYCsYnS","executionInfo":{"status":"ok","timestamp":1656599380141,"user_tz":240,"elapsed":149,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}},"outputId":"eab7bfb0-331c-41be-dbef-de389fa6882c"},"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16384,)"]},"metadata":{},"execution_count":108}]},{"cell_type":"code","source":["# do the sample distributions line up with the\n","# probabilities?\n","probs = nn.softmax(logits)\n","probs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXaA_iuwt31m","executionInfo":{"status":"ok","timestamp":1656599381013,"user_tz":240,"elapsed":116,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}},"outputId":"180d9397-c06e-426a-89ec-e8e13e0e895c"},"execution_count":109,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray([0.10999867, 0.08163206, 0.01152029, 0.01606809, 0.01451408,\n","             0.02626094, 0.01347903, 0.09747812, 0.02940934, 0.0370106 ,\n","             0.15592505, 0.03347949, 0.07056507, 0.19688964, 0.04058759,\n","             0.06518198], dtype=float32)"]},"metadata":{},"execution_count":109}]},{"cell_type":"code","source":["# they do!\n","samples_oh = nn.one_hot(samples, num_choices)\n","samples_counts = jnp.sum(samples_oh, axis=0)\n","samples_dist = samples_counts / num_samples\n","np.abs(probs - samples_dist)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdLQILojvoaF","executionInfo":{"status":"ok","timestamp":1656599382064,"user_tz":240,"elapsed":100,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}},"outputId":"cfad78e7-c63a-4670-b2b1-bc689d8c4063"},"execution_count":110,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([9.2884898e-04, 8.2150847e-04, 7.6388009e-05, 2.8932840e-04,\n","       4.1496195e-04, 2.0593740e-03, 1.9018287e-03, 1.4087856e-03,\n","       4.7867931e-04, 8.4325671e-05, 3.7644058e-03, 1.7988309e-03,\n","       8.4606558e-04, 1.4746189e-03, 2.4493411e-04, 7.8988820e-04],\n","      dtype=float32)"]},"metadata":{},"execution_count":110}]},{"cell_type":"code","source":["# so now we know how to generate a random distribution over\n","# categorical choices and sample from it, assume we want to model\n","# mapping from x_choices to y_choices, we will need one row per\n","# x_choice and one column per y_choice\n","x_choices = 16\n","y_choices = 8"],"metadata":{"id":"Gv4cb3BPwhgD","executionInfo":{"status":"ok","timestamp":1656599391997,"user_tz":240,"elapsed":106,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["# and to do a mapping, select the distribution and sample from it\n","# call the mapping w for weights, we will \n","@jit\n","def predict(rng, x, w):\n","    logits = w[x]\n","    return random.categorical(rng, logits)"],"metadata":{"id":"jGm7lC6QxUIS","executionInfo":{"status":"ok","timestamp":1656599392777,"user_tz":240,"elapsed":136,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["# now let's put it together and see if we can learn the weights\n","# generate the weights\n","rng, r = random.split(rng)\n","w = random.normal(r, (x_choices, y_choices))"],"metadata":{"id":"ILZAvEzRxUyQ","executionInfo":{"status":"ok","timestamp":1656599393905,"user_tz":240,"elapsed":217,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["# create a sampler to generate a training pair\n","@jit\n","def sample(rng, w):\n","    rx, ry = random.split(rng)\n","    x = random.randint(rx, minval=0, maxval=x_choices, shape=())\n","    y = predict(ry, x, w)\n","    return x, y\n","\n","rng, r = random.split(rng)\n","sample(r, w)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uUGkGF1d7Sum","executionInfo":{"status":"ok","timestamp":1656599395064,"user_tz":240,"elapsed":794,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}},"outputId":"9640a55c-ce62-4548-f366-004e3bfca805"},"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(DeviceArray(3, dtype=int32), DeviceArray(1, dtype=int32))"]},"metadata":{},"execution_count":115}]},{"cell_type":"code","source":["# define the loss function, actually, this isn't going to work\n","# our loss function needs to be differentiable and the predict function\n","# isn't differentiable, what would the loss even be? when\n","# we call predict, we get a single categorical value back\n","# how would you compute the loss in a way that connects\n","# back to the weights?\n","\n","# I think this is where reparamaterization trick(s) come in"],"metadata":{"id":"xy4ymHDo9T52","executionInfo":{"status":"ok","timestamp":1656599610056,"user_tz":240,"elapsed":127,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["# So let's take a step back and just try to learn a normal distribution\n","# mean and variance"],"metadata":{"id":"TLMlX1F9BLn8","executionInfo":{"status":"ok","timestamp":1656599644727,"user_tz":240,"elapsed":100,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["mean = 1.\n","std = 2.5\n","rng, r = random.split(rng)\n","z = random.normal(r)\n","mean + std * z"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w95Ri2eiCIlT","executionInfo":{"status":"ok","timestamp":1656599806003,"user_tz":240,"elapsed":377,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}},"outputId":"3a6dc0b9-148a-415f-fe38-aaa00ebfff22"},"execution_count":120,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray(-5.5664186, dtype=float32)"]},"metadata":{},"execution_count":120}]},{"cell_type":"code","source":["# run this a bunch and see if we get back mean and std from the samples\n","num_samples = 10_000\n","rng, r = random.split(rng)\n","zs = random.normal(r, shape=(num_samples,))\n","ys = mean + std * zs\n","jnp.mean(ys), jnp.std(ys)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfbITOtMCeXn","executionInfo":{"status":"ok","timestamp":1656599899478,"user_tz":240,"elapsed":446,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}},"outputId":"56f94821-a81c-4500-affd-a5db282cd5e6"},"execution_count":124,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(DeviceArray(1.0074186, dtype=float32), DeviceArray(2.508156, dtype=float32))"]},"metadata":{},"execution_count":124}]},{"cell_type":"code","source":["# ok so the problem is that random.normal isn't differentiable\n","# since it's a random function. But, the call to random.normal itself\n","# doesn't actually depend on our weights (mean, std) so we can come up\n","# with a loss function that is differentiable and pulls the mean and\n","# std in the right direction"],"metadata":{"id":"sTJcmpo-DGsH","executionInfo":{"status":"ok","timestamp":1656600015165,"user_tz":240,"elapsed":104,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":125,"outputs":[]},{"cell_type":"code","source":["# let's clean this up for a second, our\n","# weights will be a mean and std\n","GaussianModel = namedtuple('GaussianModel', 'mean std')\n","w = GaussianModel(mean=1., std=2.5)"],"metadata":{"id":"WCAzFSqtDjBP","executionInfo":{"status":"ok","timestamp":1656600145080,"user_tz":240,"elapsed":85,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":127,"outputs":[]},{"cell_type":"code","source":["# our sample function will perform the standard sampling from the\n","# above, \n","@jit\n","def sample(rng, w):\n","    z = random.normal(rng, shape=())\n","    return w.mean + w.std * z\n","rng, r = random.split(rng)\n","sample(r, w)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aDM85RCGECvO","executionInfo":{"status":"ok","timestamp":1656600348592,"user_tz":240,"elapsed":234,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}},"outputId":"c1e16b6a-6351-4676-c08f-18fa59eae1ad"},"execution_count":137,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray(1.8244866, dtype=float32)"]},"metadata":{},"execution_count":137}]},{"cell_type":"code","source":["# this is differentiable wrt the weights\n","rng, r = random.split(rng)\n","grad(partial(sample, r))(w)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"loG9TKoTEXYl","executionInfo":{"status":"ok","timestamp":1656600466438,"user_tz":240,"elapsed":322,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}},"outputId":"a5f50c2b-5b54-49af-a74a-5311e93bd792"},"execution_count":143,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GaussianModel(mean=DeviceArray(1., dtype=float32, weak_type=True), std=DeviceArray(0.29066396, dtype=float32, weak_type=True))"]},"metadata":{},"execution_count":143}]},{"cell_type":"code","source":["# now we need a loss that will pull the mean and std\n","# in the right direction, this is the actual trick in a sense, but it's not\n","# really a trick is it? Or is the trick the fact that we can call sample\n","# in a way that the random part is separate from the part that relies on the\n","# weights?"],"metadata":{"id":"Yz8huGeOElG9","executionInfo":{"status":"ok","timestamp":1656600522702,"user_tz":240,"elapsed":118,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":144,"outputs":[]},{"cell_type":"code","source":["# so we need to implement some loss function, which is a bunch of math \n","# from https://arxiv.org/pdf/1312.6114v10.pdf "],"metadata":{"id":"8RR3khkXFe7e","executionInfo":{"status":"ok","timestamp":1656601779286,"user_tz":240,"elapsed":101,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":146,"outputs":[]},{"cell_type":"code","source":["# Maybe as a first step, how would I learn this as a per-sample\n","# algorithm, first manually in a loop through samples and then\n","# expressed as a gradient, take a look at the formula here:\n","# https://stats.stackexchange.com/questions/365192/bayesian-update-for-a-univariate-normal-distribution-with-unknown-mean-and-varia"],"metadata":{"id":"sNpeW4cGKajO","executionInfo":{"status":"ok","timestamp":1656602827981,"user_tz":240,"elapsed":89,"user":{"displayName":"Maurice Flanagan","userId":"01067774167747843791"}}},"execution_count":148,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lb0wI1M9ORvu"},"execution_count":null,"outputs":[]}]}