{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "Gradients play a critical role in machine learning and most modern machine learning frameworks provide tools for computing gradients automatically, reducing the need to work out the derivatives by hand. This notebook demonstrates several approaches to computing gradients using [numerical differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation), [symbolic differentiation](https://en.wikipedia.org/wiki/Computer_algebra) and [TensorFlow gradients](https://www.tensorflow.org/api_docs/python/tf/gradients). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving formulas by hand \n",
    "Our goal is to compute a first order and second order partial derivative for a simple [polynomial](https://en.wikipedia.org/wiki/Polynomial) $f = 2ab^2 + 3a^2b$. Specifically we want to evaluate the polynomial and its derivatives $\\frac{\\partial f}{\\partial a}$ and $\\frac{\\partial^2 f}{\\partial a \\partial b}$ at $a = 2$ and $b = 3$. The first step is to work out the derivatives by hand so that we can test our automated implementations, we will apply the following basic calculus rules:\n",
    "\n",
    "**Sum Rule**\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "f &= g + h \\\\\n",
    "\\frac{\\partial f}{\\partial x} &= \\frac{\\partial g}{\\partial x} + \\frac{\\partial h}{\\partial x}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "**Product Rule**\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "f &= gh \\\\\n",
    "\\frac{\\partial f}{\\partial x} &= g\\frac{\\partial h}{\\partial x} + g\\frac{\\partial h}{\\partial x}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "**Power Rule**\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "f &= g^{n} \\\\\n",
    "\\frac{\\partial f}{\\partial x} &= ng^{n-1}\\frac{\\partial g}{\\partial x}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the above rules, we get:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "f &= 2ab^2 + 3a^2b \\\\\n",
    "\\frac{\\partial f}{\\partial a} &= 2b^2 + 6ab \\\\\n",
    "\\frac{\\partial^2 f}{\\partial a \\partial b} &= 4b + 6a \\\\\n",
    "\\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we worked out the derivatives, let's evaluate them at $a = 2$ and $b = 3$ so we have some test data when evaluating our automated approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 54, 24)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def py_f(a, b):\n",
    "    return 2 * a * (b ** 2) + 3 * (a ** 2) * b\n",
    "\n",
    "def py_f_da(a, b):\n",
    "    return 2 * (b ** 2) + 6 * a * b\n",
    "\n",
    "def py_f_da_db(a, b):\n",
    "    return 4 * b + 6 * a\n",
    "\n",
    "py_f(a=2, b=3), py_f_da(a=2, b=3), py_f_da_db(a=2, b=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical differentiation\n",
    "Our first attempt at automatic differentiation is through [numerical differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation) inspired by the limit formula for derivatives:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial x} = \\lim _{h\\to 0}{\\frac {f(x+h)-f(x)}{h}}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this definition, we can approximate the derivative by calculating the function at $x$ and $x+h$ and calcluating the slope between the two points, this makes sense because the derivative is the slope of the curve in the region of $x$. Our formula will be slightly more complicated because we are computing a partial derivative so we only want to apply the $h$ offset to the variable we are taking the derivative with respect to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_partial(f, darg, h=.0001):\n",
    "    def df(**kwargs):\n",
    "        dkwargs = kwargs.copy()\n",
    "        dkwargs[darg] += h\n",
    "        return (f(**dkwargs) - f(**kwargs)) / h\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `numerical_partial` function generates a new function which will calculate the partial derivative of `f` with respect to `darg`, let's use this to generate our partial derivative functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_f_da = numerical_partial(py_f, 'a')\n",
    "n_f_da_db = numerical_partial(n_f_da, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to evaluate our partial derivatives, we see they are very close to the exact partials we found earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 54.00090000009072, 24.000500786769408)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_f(a=2, b=3), n_f_da(a=2, b=3), n_f_da_db(a=2, b=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although numerical differentiation is simple and relatively easy to implement, it is computationally expensive because of the need to compute the underlying function `f` multiple times, this get's worse for higher order partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic differentiation\n",
    "Our second approach will be [symbolic differentiation](https://en.wikipedia.org/wiki/Computer_algebra). This approach involves representing the function `f` as an [expression](https://en.wikipedia.org/wiki/Expression_(computer_science) and implementing an `sevaluate` method which can calculate a value of the expression for a particular set of variable values and an `sderivative` which transforms the expression into another expression based on the rules of calculus. We will implement a very simple, highly restricted expression and methods which only works with simple polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent polynomials as nested `tuple` objects, for example, our polynomial $f = 2ab^2 + 3a^2b$ is represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_f = ((2, ('a', 1), ('b', 2)), (3, ('a', 2), ('b', 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is a very limited, toy example which was chosen just because its simple and gets the job done for this example. Now that we have a format, let's implement `sevaluate` and see if we can calculate the value of this function at $a=2$ and $b=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sevaluate(expression, variable_values):\n",
    "    total = 0\n",
    "    for term in expression:\n",
    "        coeff, *variables = term\n",
    "        term_total = coeff\n",
    "        for variable, exponent in variables:\n",
    "            variable_value = variable_values[variable]\n",
    "            term_total *= variable_value ** exponent\n",
    "        total += term_total\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sevaluate(s_f, {'a': 2, 'b':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluate function appears to work, now we need a way to convert that expression into another expression which represents the partial derivative, we will implement `sderivative` which will perform the transformation. Again, this function is just a toy and can only deal with this very limited case of a simple polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sderivative(expression, dvariable):\n",
    "    dexpresion = []\n",
    "    for term in expression:\n",
    "        coeff, *variables = term\n",
    "        dterm = []\n",
    "        for variable, exponent in variables:\n",
    "            if variable == dvariable:\n",
    "                coeff *= exponent\n",
    "                exponent -= 1\n",
    "            dterm.append((variable, exponent))\n",
    "        dterm.insert(0, coeff)\n",
    "        dexpresion.append(tuple(dterm))\n",
    "    return tuple(dexpresion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our new method to compute the expression corresponding to both of the partial derivatives we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, ('a', 0), ('b', 2)), (6, ('a', 1), ('b', 1)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_f_da = sderivative(s_f, 'a')\n",
    "s_f_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, ('a', 0), ('b', 1)), (6, ('a', 1), ('b', 0)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_f_da_db = sderivative(s_f_da, 'b')\n",
    "s_f_da_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above expressions appear correct, let's test it out and see if we get the three values we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 54, 24)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sevaluate(s_f, {'a': 2, 'b': 3}), sevaluate(s_f_da, {'a': 2, 'b': 3}), sevaluate(s_f_da_db, {'a': 2, 'b': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our toy system for computing symbolic derivatives worked and gave us exact answers without requiring us to work out the derivatives by hand. Our final example shows how TensorFlow can be used to compute exact gradients automatically through a similar (but vastly more sohpisticated) computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "In our final example we will use TensorFlow to represent our function `f` as a Tensor and use the `tf.gradients` method to generate gradient tensors. The first step is to create a tensor for our polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(shape=(), dtype=float, name='a')\n",
    "b = tf.placeholder(shape=(), dtype=float, name='b')\n",
    "t_f = 2 * a * (b ** 2) + 3 * (a ** 2) * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that TensorFlow used operator overloading to build up a computation graph and store a handle to the graph in `t_f`, we haven't calculated any values yet. Now let's use `tf.gradients` to get our partial derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_f_da = tf.gradients(t_f, a)[0]\n",
    "t_f_da_db = tf.gradients(t_f_da, b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to calculate our values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72.0, 54.0, 24.0)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run((t_f, t_f_da, t_f_da_db), feed_dict={a:2, b:3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrated some approaches to automatic differentiation which is a core feature of most machine learning frameworks. By providing a way to automatically compute gradients, modern machine learning frameworks make it much simpler to experiment with different activation functions and neural network topologies without needing to work out the potentially complex math involved in generating a gradient for backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
