{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mauricef/jax-ml/blob/main/actor-critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqFSCo5hXO6Y"
   },
   "outputs": [],
   "source": [
    "%pip install dm-haiku optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoaKFt-nXJtc",
    "tags": []
   },
   "source": [
    "# Jax Actor Critic\n",
    "This is an implementation of the [TensorFlow actor critic example](https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic) using JAX, Haiku and Optax. The TF version executes ~ 15 it/sec while the JAX version executes ~ 35 it/sec on the same CPU Colab. They are both much slower using GPU acceleration.\n",
    "\n",
    "\n",
    "The code is similar between the two implementations but there are some important differences.\n",
    "\n",
    "- The JAX version generates fixed length episodes with a reward of `np.nan` when the environment enters the `done` state. The TF version generates variable length episodes. If we generate variable length episodes then the `jit` on the `train_step` will keep triggering a recompile every time it encounters a different episode length which seriously slows things down.\n",
    "\n",
    "- Due to the fixed episode length, we need to normalize the returns while taking into account `np.nan` values, that is what the `safe_*` methods are for.\n",
    "\n",
    "- Also due to the variable length episodes, we need to filter out steps that are past the end of the episode when computing the gradient.\n",
    "\n",
    "- JAX computes gradients by tracing a function call taking the model weights as the first parameter vs TF which traces the execution using the gradient tape context. To get this working we needed to refactor the responsabilities of the `generate_episode` and `compute_loss_and_grads`. This does require two model evaluations per step - the first to sample an action in `generate_episode` and the second to compute the `value` and `policy` in `compute_loss_and_grads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RS_YDAYYXJtd"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import gym\n",
    "import haiku as hk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as onp\n",
    "import optax\n",
    "from jax import jit, lax, vmap, value_and_grad, partial\n",
    "import jax.nn as nn\n",
    "import jax.numpy as np\n",
    "import jax.random as random\n",
    "from jax.tree_util import tree_map\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iyNWe1jaXJte"
   },
   "outputs": [],
   "source": [
    "Episode = collections.namedtuple('Episode', 'state action reward value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hHT2-Og_XJte"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "gamma = .99\n",
    "num_actions = env.action_space.n  # 2\n",
    "num_hidden_units = 128\n",
    "episode_steps = 200\n",
    "optimizer = optax.adam(learning_rate=.01)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CQgoCyzOXJtf"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def scan_stack(ys):\n",
    "    return tuple(map(np.array, zip(*ys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y9wMtkd4XJtf"
   },
   "outputs": [],
   "source": [
    "def scan(f, init, xs=None, length=None):\n",
    "    if xs is None:\n",
    "        xs = [None] * length\n",
    "    carry = init\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        carry, y = f(carry, x)\n",
    "        ys.append(y)\n",
    "    return carry, scan_stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TDBtQmmFXJtg"
   },
   "outputs": [],
   "source": [
    "# nan aware helpers\n",
    "\n",
    "def safe_mean(x):\n",
    "    mask = ~np.isnan(x)\n",
    "    n = np.count_nonzero(mask)\n",
    "    total = np.sum(np.nan_to_num(x))\n",
    "    return total / n\n",
    "\n",
    "def safe_var(x):\n",
    "    return safe_mean(np.square(x)) - np.square(safe_mean(x))\n",
    "\n",
    "def safe_normalize(x, epsilon=eps):\n",
    "    mean = safe_mean(x)\n",
    "    var = safe_var(x)\n",
    "    return (x - mean) * lax.rsqrt(var + epsilon)\n",
    "\n",
    "def safe_npv(rate, xs):\n",
    "    def step(total, x):\n",
    "        total = x + rate * total\n",
    "        return total, total\n",
    "    mask = ~np.isnan(xs)\n",
    "    xs = np.nan_to_num(xs)\n",
    "    _, ys = lax.scan(step, init=0., xs=xs, reverse=True)\n",
    "    ys = np.where(mask, ys, np.nan)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qoh4aLpSXJtg"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_values(rewards, gamma=gamma):\n",
    "    values = safe_npv(gamma, rewards)\n",
    "    return safe_normalize(values, epsilon=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RQDWAeoOXJth"
   },
   "outputs": [],
   "source": [
    "def env_is_done(env):\n",
    "    return env.steps_beyond_done is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SlKN91OsXJth"
   },
   "outputs": [],
   "source": [
    "def env_init(rng):\n",
    "    seed = random.randint(r, shape=(), minval=0, maxval=np.iinfo(np.int32).max)\n",
    "    env.seed(int(seed))\n",
    "    return np.array(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ytr0sNbcXJti"
   },
   "outputs": [],
   "source": [
    "def env_step(action):\n",
    "    if env_is_done(env):\n",
    "        state, reward = onp.array(env.state), np.nan\n",
    "    else:\n",
    "        state, reward, _, _ = env.step(int(action))\n",
    "    return state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "spvIs0Q6XJti"
   },
   "outputs": [],
   "source": [
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def model(x):\n",
    "    x = hk.Linear(num_hidden_units, name='common')(x)\n",
    "    x = nn.relu(x)\n",
    "    actor = hk.Linear(2, name='actor')(x)\n",
    "    critic = hk.Linear(1, name='critic')(x)\n",
    "    return actor, critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DgELYUREXJti"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def sample_policy(rng, model_state, state):\n",
    "    policy, _ = model.apply(model_state, state)\n",
    "    return random.categorical(rng, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6Z2Q23nqXJtj"
   },
   "outputs": [],
   "source": [
    "def generate_episode(rng, model_state):\n",
    "    def generate_step(state, rng):\n",
    "        action = sample_policy(rng, model_state, state)\n",
    "        next_state, reward = env_step(action)\n",
    "        return next_state, (state, action, reward)\n",
    "\n",
    "    rng, r = random.split(rng)\n",
    "    initial_state = env_init(r)\n",
    "    rngs = np.array(random.split(rng, episode_steps))\n",
    "    _, (state, action, reward) = scan(generate_step, initial_state, xs=rngs)\n",
    "    value = get_values(reward)\n",
    "    return Episode(state, action, reward, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S7w2wl4VXJtj"
   },
   "outputs": [],
   "source": [
    "def huber_loss(yp, y, delta=1.):\n",
    "    residual = np.abs(y - yp)\n",
    "    return np.where(residual < delta, .5 * residual ** 2, residual - .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IBGIP0ToXJtk"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss_fn(model_state, step):\n",
    "    policy_logits, predicted_value = model.apply(model_state, step.state)\n",
    "    advantage = step.value - predicted_value\n",
    "    policy_probs = nn.softmax(policy_logits)\n",
    "    action_prob = policy_probs[step.action]\n",
    "    action_log_prob = np.log(action_prob)\n",
    "    actor_loss = -action_log_prob * advantage\n",
    "    critic_loss = huber_loss(predicted_value, step.value)\n",
    "    loss = actor_loss + critic_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CUUMwwtTXJtk"
   },
   "outputs": [],
   "source": [
    "@value_and_grad\n",
    "def compute_loss_and_grads(model_state, episode):\n",
    "    def compute_loss_step(step):\n",
    "        w = tree_map(lambda a: np.where(np.isnan(step.reward), lax.stop_gradient(a), a), model_state)\n",
    "        loss = loss_fn(w, step)\n",
    "        loss = np.nan_to_num(loss)\n",
    "        return loss\n",
    "    losses = vmap(partial(compute_loss_step))(episode)\n",
    "    return np.sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Yiat4_olXJtk"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step(episode, model_state, opt_state):\n",
    "    loss_value, loss_grads = compute_loss_and_grads(model_state, episode)\n",
    "    model_updates, opt_state = optimizer.update(loss_grads, opt_state, model_state)\n",
    "    model_state = optax.apply_updates(model_state, model_updates)\n",
    "    episode_reward = np.sum(np.nan_to_num(episode.reward))\n",
    "    return episode_reward, loss_value, model_state, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAZCtUoOXJtk",
    "outputId": "39abc052-7eb1-4692-c370-ac33d22cd4e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "rng = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "054duhjcXJtl",
    "outputId": "6d9d1c65-c499-4b1e-ca7b-beba0629a17f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1255:  13%|█▎        | 1255/10000 [00:31<03:40, 39.73it/s, episode_reward=200.0, running_reward=195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 1255: average reward: 195.14!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rng, r = random.split(rng)\n",
    "model_state = model.init(r, env_init(r))\n",
    "opt_state = optimizer.init(model_state)\n",
    "\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 200\n",
    "min_episodes_criterion = 100\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "gamma = 0.99\n",
    "episodes_reward = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    for i in t:\n",
    "        rng, r = random.split(rng)\n",
    "        episode = generate_episode(r, model_state)\n",
    "        rng, r = random.split(rng)\n",
    "        episode_reward, loss_value, model_state, opt_state = train_step(episode, model_state, opt_state)\n",
    "        episodes_reward.append(episode_reward)\n",
    "        running_reward = onp.mean(episodes_reward)\n",
    "        \n",
    "        t.set_description(f'Episode {i}')\n",
    "        t.set_postfix(\n",
    "            episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "        if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "            break\n",
    "            \n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jQ1tEQCxwRx"
   ],
   "include_colab_link": true,
   "name": "actor_critic.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
