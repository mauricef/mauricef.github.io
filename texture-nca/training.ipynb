{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MG9p2lFIEWNf",
    "outputId": "e484b239-9377-47eb-84ea-d815372cda27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "#!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NanzLO45EOX3"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet ml_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s-PV8s_dEaMO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import PIL.Image, PIL.ImageDraw, PIL.ImageFont \n",
    "import base64\n",
    "import zipfile\n",
    "import json\n",
    "#import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from string import Template\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import Image, HTML, clear_output, Javascript\n",
    "\n",
    "from ml_collections import ConfigDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zFkaiY_FDeD"
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j0DR0GnVEqSG"
   },
   "outputs": [],
   "source": [
    "cfg = ConfigDict()\n",
    "cfg.logdir = \"/tmp/ca\" # base log dir\n",
    "cfg.experiment_name = \"0\" # directory under log_dir to place output.\n",
    "cfg.ffmpeg_path = \"ffmpeg\"\n",
    "\n",
    "try:\n",
    "  __IPYTHON__\n",
    "  cfg.is_ipython = True\n",
    "except NameError:\n",
    "  cfg.is_ipython = False\n",
    "\n",
    "cfg.texture_ca = ConfigDict()\n",
    "cfg.texture_ca.channel_n = 12\n",
    "cfg.texture_ca.hidden_n = 96\n",
    "cfg.texture_ca.fire_rate = 0.5\n",
    "cfg.texture_ca.batch_size = 4\n",
    "cfg.texture_ca.lr = 2e-3\n",
    "cfg.texture_ca.pool_size = 1024\n",
    "cfg.texture_ca.fixed_seed = 123 # 0 to disable\n",
    "cfg.texture_ca.lr = 2e-3\n",
    "cfg.texture_ca.lr_decay = 2000\n",
    "cfg.texture_ca.rollout_len_min = 32\n",
    "cfg.texture_ca.rollout_len_max = 64\n",
    "cfg.texture_ca.train_steps = 2000\n",
    "cfg.texture_ca.gradnorm = True\n",
    "cfg.texture_ca.q = 2.0\n",
    "cfg.texture_ca.bias = True\n",
    "cfg.texture_ca.learned_filters = 0\n",
    "cfg.texture_ca.laplacian = True\n",
    "cfg.texture_ca.gradient = True\n",
    "cfg.texture_ca.identity = True\n",
    "\n",
    "# texture synth / style transfer\n",
    "cfg.texture_ca.ancestor_npy = ''\n",
    "cfg.texture_ca.img_size = 128\n",
    "cfg.texture_ca.vgg_input_img_size = 128\n",
    "cfg.texture_ca.texture_dir = 'textures'\n",
    "cfg.texture_ca.ancestor_dir = 'models'\n",
    "cfg.texture_ca.objective = \"style:mondrian.jpg\" #{style:mondrian.jpg, inception:mixed4b_pool_reduce_pre_relu:30}\n",
    "cfg.texture_ca.inception_pb = 'gs://modelzoo/vision/other_models/InceptionV1.pb'\n",
    "cfg.texture_ca.hidden_viz_group = False # Group the hidden states into RGB when vizualizing\n",
    "cfg.texture_ca.viz_rollout_len = 1000\n",
    "cfg.texture_ca.overflow_loss_coef = 1e4 # auxiliary loss to keep generated values in [0,1]\n",
    "tcfg = cfg.texture_ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TCN1baYFFWX"
   },
   "source": [
    "### Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ypurZI3hFGcm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.io.gfile import GFile\n",
    "import io\n",
    "import base64\n",
    "import requests\n",
    "import PIL.Image, PIL.ImageDraw\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def imread(url, max_size=None, mode=None):\n",
    "  if isinstance(url, str):\n",
    "    if url.startswith(('http:', 'https:')):\n",
    "      r = requests.get(url)\n",
    "      f = io.BytesIO(r.content)\n",
    "    else:\n",
    "      f = GFile(url, mode='rb')\n",
    "  else:\n",
    "    f = url\n",
    "  img = PIL.Image.open(f)\n",
    "  if max_size is not None:\n",
    "    img.thumbnail((max_size, max_size), PIL.Image.ANTIALIAS)\n",
    "  if mode is not None:\n",
    "    img = img.convert(mode)\n",
    "  img = np.float32(img)/255.0\n",
    "  return img\n",
    "\n",
    "\n",
    "def np2pil(a):\n",
    "  if a.dtype in [np.float32, np.float64]:\n",
    "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
    "  return PIL.Image.fromarray(a)\n",
    "\n",
    "def imwrite(f, a, fmt=None):\n",
    "  a = np.asarray(a)\n",
    "  if isinstance(f, str):\n",
    "    fmt = f.rsplit('.', 1)[-1].lower()\n",
    "    if fmt == 'jpg':\n",
    "      fmt = 'jpeg'\n",
    "    f = GFile(f, mode='wb')\n",
    "  np2pil(a).save(f, fmt, quality=95)\n",
    "\n",
    "def imencode(a, fmt='jpeg'):\n",
    "  a = np.asarray(a)\n",
    "  if len(a.shape) == 3 and a.shape[-1] == 4:\n",
    "    fmt = 'png'\n",
    "  f = io.BytesIO()\n",
    "  imwrite(f, a, fmt)\n",
    "  return f.getvalue()\n",
    "\n",
    "def imshow(a, fmt='jpeg'):\n",
    "  from IPython.display import Image, display\n",
    "  display(Image(data=imencode(a, fmt)))\n",
    "\n",
    "def im2url(a, fmt='jpeg'):\n",
    "  encoded = imencode(a, fmt)\n",
    "  base64_byte_string = base64.b64encode(encoded).decode('ascii')\n",
    "  return 'data:image/' + fmt.upper() + ';base64,' + base64_byte_string\n",
    "\n",
    "def tile2d(a, w=None):\n",
    "  a = np.asarray(a)\n",
    "  if w is None:\n",
    "    w = int(np.ceil(np.sqrt(len(a))))\n",
    "  th, tw = a.shape[1:3]\n",
    "  pad = (w-len(a))%w\n",
    "  a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')\n",
    "  h = len(a)//w\n",
    "  a = a.reshape([h, w]+list(a.shape[1:]))\n",
    "  a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))\n",
    "  return a\n",
    "\n",
    "def zoom(img, scale=4):\n",
    "  img = np.repeat(img, scale, 0)\n",
    "  img = np.repeat(img, scale, 1)\n",
    "  return img\n",
    "\n",
    "class VideoWriter:\n",
    "  def __init__(self, filename, fps=30.0, **kw):\n",
    "    self.writer = None\n",
    "    self.params = dict(filename=filename, fps=fps, **kw)\n",
    "\n",
    "  def add(self, img):\n",
    "    img = np.asarray(img)\n",
    "    if self.writer is None:\n",
    "      h, w = img.shape[:2]\n",
    "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
    "    if img.dtype in [np.float32, np.float64]:\n",
    "      img = np.uint8(img.clip(0, 1)*255)\n",
    "    if len(img.shape) == 2:\n",
    "      img = np.repeat(img[..., None], 3, -1)\n",
    "    self.writer.write_frame(img)\n",
    "\n",
    "  def close(self):\n",
    "    if self.writer:\n",
    "      self.writer.close()\n",
    "\n",
    "  def __enter__(self):\n",
    "    return self\n",
    "\n",
    "  def __exit__(self, *kw):\n",
    "    self.close()\n",
    "\n",
    "  def show(self, **kw):\n",
    "      self.close()\n",
    "      fn = self.params['filename']\n",
    "      display(mvp.ipython_display(fn, **kw))\n",
    "\n",
    "class Bunch(dict):\n",
    "  \"\"\"Dot-accessible dict.\"\"\"\n",
    "  def __init__(self, *args, **kwds):\n",
    "    super(Bunch, self).__init__(*args, **kwds)\n",
    "    self.__dict__ = self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfZny_kAFZEc"
   },
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "79PbvtpjFX3y"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "style_layers = ['block%d_conv1'%i for i in range(1, 6)]\n",
    "content_layer = 'block4_conv2'\n",
    "\n",
    "class StyleModel:\n",
    "  def __init__(self, input_texture_path):\n",
    "    vgg = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "    layers = style_layers + [content_layer]\n",
    "    layers = {name:vgg.get_layer(name).output for name in layers}\n",
    "    self.model = tf.keras.Model([vgg.input], layers)\n",
    "    self.style_img = imread(input_texture_path, cfg.texture_ca.vgg_input_img_size)\n",
    "    self.target_style, _ = self.calc_style_content(self.style_img[None,...])\n",
    "\n",
    "  def run_model(self, img):\n",
    "    img = img[..., ::-1]*255.0 - np.float32([103.939, 116.779, 123.68])\n",
    "    layers = self.model(img)\n",
    "    style = [layers[name] for name in style_layers]\n",
    "    return style, layers[content_layer]\n",
    "\n",
    "  def calc_style_content(self, img):\n",
    "    style_layers, content = self.run_model(img)\n",
    "    style = [self.gram_style(a) for a in style_layers]\n",
    "    return style, content\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x):\n",
    "    gs, content = self.calc_style_content(x)\n",
    "    sl = tf.reduce_mean(self.style_loss(gs, self.target_style))\n",
    "    return sl\n",
    "\n",
    "  @tf.function\n",
    "  def style_loss(self, a, b):\n",
    "    return tf.add_n([tf.reduce_mean(tf.square(x-y), [-2, -1]) for x, y in zip(a, b)])\n",
    "\n",
    "  def gram_style(self, a):\n",
    "    n, h, w, ch = tf.unstack(tf.shape(a))\n",
    "    a = tf.sqrt(a+1.0)-1.0\n",
    "    gram = tf.einsum('bhwc, bhwd -> bcd', a, a)\n",
    "    return gram / tf.cast(h*w, tf.float32)\n",
    "\n",
    "class Inception:\n",
    "  def __init__(self, layer, ch):\n",
    "    with tf.io.gfile.GFile(cfg.texture_ca.inception_pb, 'rb') as f:\n",
    "      self.graph_def = tf.compat.v1.GraphDef.FromString(f.read())\n",
    "    self.layer = layer\n",
    "    self.ch = ch\n",
    "    avgpool0_idx = [n.name for n in self.graph_def.node].index('avgpool0')\n",
    "    del self.graph_def.node[avgpool0_idx:]\n",
    "    # use pre_relu layers for Concat nodes\n",
    "    node = {n.name:n for n in self.graph_def.node}[layer]\n",
    "    self.outputs = [layer+':0']\n",
    "    if 'Concat' in node.op:\n",
    "      self.outputs = [inp+'_pre_relu:0' for inp in node.input[1:]]\n",
    "  \n",
    "  @tf.function\n",
    "  def __call__(self, x):\n",
    "    overflow_loss = tf.reduce_mean(tf.square(tf.clip_by_value(x, 0.0, 1.0)-x))\n",
    "    imgs = x*255.0-117.0\n",
    "    outputs = tf.import_graph_def(self.graph_def, {'input':imgs}, self.outputs)\n",
    "    a = tf.concat(outputs, -1)\n",
    "    return -tf.reduce_mean(a[...,self.ch]) + overflow_loss*cfg.texture_ca.overflow_loss_coef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqF1C7sOIbAN"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "upMYpIKPIglL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "def pad_repeat(x, pad):\n",
    "  x = tf.concat([x[:, -pad:], x, x[:, :pad]], 1)\n",
    "  x = tf.concat([x[:, :, -pad:], x, x[:, :, :pad]], 2)\n",
    "  return x\n",
    "\n",
    "def get_variables(f):\n",
    "  '''Get all vars involved in computing a function. Userful for'''\n",
    "  with tf.GradientTape() as g:\n",
    "    f()\n",
    "    return g.watched_variables()\n",
    "\n",
    "def fake_quant(x, min, max):\n",
    "  y = tf.quantization.fake_quant_with_min_max_vars(x, min=min, max=max)\n",
    "  return y\n",
    "\n",
    "def fake_param_quant(w):\n",
    "  bound = tf.stop_gradient(tf.reduce_max(tf.abs(w)))\n",
    "  w = fake_quant(w, -bound, bound)\n",
    "  return w\n",
    "\n",
    "def to_rgb(x):\n",
    "  return x[..., :3]/(cfg.texture_ca.q) + 0.5\n",
    "\n",
    "@tf.function\n",
    "def perceive(x, angle=0.0, repeat=True):\n",
    "  chn = tf.shape(x)[-1]\n",
    "  identify = np.outer([0, 1, 0], [0, 1, 0])\n",
    "  dx = np.outer([1, 2, 1], [-1, 0, 1]) / 8.0  # Sobel filter\n",
    "  dy = dx.T\n",
    "  laplacian = np.outer([1, 2, 1], [1, 2, 1]) / 8.0\n",
    "  laplacian[1, 1] -= 2.0\n",
    "  c, s = tf.cos(angle), tf.sin(angle)\n",
    "  kernel = tf.stack([identify, c*dx-s*dy, s*dx+c*dy, laplacian], -1)[:, :, None, :]\n",
    "  kernel = tf.repeat(kernel, chn, 2)\n",
    "  pad_mode = 'SAME'\n",
    "  if repeat:\n",
    "    x = pad_repeat(x, 1)\n",
    "    pad_mode = 'VALID'\n",
    "  y = tf.nn.depthwise_conv2d(x, kernel, [1, 1, 1, 1], pad_mode)\n",
    "  return y\n",
    "\n",
    "class DenseLayer:\n",
    "  def __init__(self, in_n, out_n,\n",
    "              init_fn=tf.initializers.glorot_uniform()):\n",
    "    w0 = tf.concat([init_fn([in_n, out_n]), tf.zeros([1, out_n])], 0)\n",
    "    self.w = tf.Variable(w0)\n",
    "\n",
    "  def embody(self):\n",
    "    w = fake_param_quant(self.w)\n",
    "    w, b = w[:-1], w[-1]\n",
    "    w = w[None, None, ...]\n",
    "    def f(x):\n",
    "      # TFjs matMul doesn't work with non-2d tensors, so using\n",
    "      # conv2d instead of 'tf.matmul(x, w)+b'\n",
    "      return tf.nn.conv2d(x, w, 1, 'VALID')+b\n",
    "    return f\n",
    "\n",
    "class CAModel:\n",
    "\n",
    "  def __init__(self, params=None):\n",
    "    super().__init__()\n",
    "    self.fire_rate = cfg.texture_ca.fire_rate\n",
    "    self.channel_n = cfg.texture_ca.channel_n\n",
    "\n",
    "    init_fn = tf.initializers.glorot_normal(cfg.texture_ca.fixed_seed or None)\n",
    "    self.layer1 = DenseLayer(self.channel_n*4, cfg.texture_ca.hidden_n, init_fn)\n",
    "    self.layer2 = DenseLayer(cfg.texture_ca.hidden_n, self.channel_n, tf.zeros)\n",
    "\n",
    "    self.params = get_variables(self.embody)\n",
    "    if params is not None:\n",
    "      self.set_params(params)\n",
    "\n",
    "  def embody(self, quantized=True):\n",
    "    layer1 = self.layer1.embody()\n",
    "    layer2 = self.layer2.embody()\n",
    "\n",
    "    def noquant(x, min, max):\n",
    "      return tf.clip_by_value(x, min, max)\n",
    "    qfunc = fake_quant if quantized else noquant\n",
    "\n",
    "    @tf.function\n",
    "    def f(x, fire_rate=None, angle=0.0, step_size=1.0):\n",
    "      y = perceive(x, angle)\n",
    "      y = qfunc(y, min=-cfg.texture_ca.q, max=cfg.texture_ca.q)\n",
    "      y = tf.nn.relu(layer1(y))\n",
    "      y = qfunc(y, min=0.0, max=cfg.texture_ca.q)\n",
    "      y = layer2(y)\n",
    "      dx = y*step_size\n",
    "      dx = qfunc(dx, min=-cfg.texture_ca.q, max=cfg.texture_ca.q)\n",
    "      if fire_rate is None:\n",
    "        fire_rate = self.fire_rate\n",
    "      update_mask = tf.random.uniform(tf.shape(x[:, :, :, :1])) <= fire_rate\n",
    "      x += dx * tf.cast(update_mask, tf.float32)\n",
    "      x = qfunc(x, min=-cfg.texture_ca.q, max=cfg.texture_ca.q)\n",
    "      return x\n",
    "    return f\n",
    "\n",
    "  def get_params(self):\n",
    "    return [p.numpy() for p in self.params]\n",
    "\n",
    "  def set_params(self, params):\n",
    "    for v, p in zip(self.params, params):\n",
    "      v.assign(p)\n",
    "\n",
    "  def save_params(self, filename):\n",
    "    with tf.io.gfile.GFile(filename, mode='wb') as f: \n",
    "      np.save(f, self.get_params())\n",
    "\n",
    "  def load_params(self, filename):\n",
    "    with tf.io.gfile.GFile(filename, mode='rb') as f: \n",
    "      params = np.load(f, allow_pickle=True)\n",
    "      self.set_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS5EotMpHHeO"
   },
   "source": [
    "### TextureSynthTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "leXfJ-goHoI_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SamplePool:\n",
    "  def __init__(self, *, _parent=None, _parent_idx=None, **slots):\n",
    "    self._parent = _parent\n",
    "    self._parent_idx = _parent_idx\n",
    "    self._slot_names = slots.keys()\n",
    "    self._size = None\n",
    "    for k, v in slots.items():\n",
    "      if self._size is None:\n",
    "        self._size = len(v)\n",
    "      assert self._size == len(v)\n",
    "      setattr(self, k, np.asarray(v))\n",
    "\n",
    "  def sample(self, n):\n",
    "    idx = np.random.choice(self._size, n, False)\n",
    "    batch = {k: getattr(self, k)[idx] for k in self._slot_names}\n",
    "    batch = SamplePool(**batch, _parent=self, _parent_idx=idx)\n",
    "    return batch\n",
    "\n",
    "  def commit(self):\n",
    "    for k in self._slot_names:\n",
    "      getattr(self._parent, k)[self._parent_idx] = getattr(self, k)\n",
    "\n",
    "def create_loss_model():\n",
    "  loss_type, loss_params = tcfg.objective.split(':', 1)\n",
    "  if loss_type == \"style\":\n",
    "    texture_fn = loss_params\n",
    "    input_texture_path = \"%s/%s\"%(tcfg.texture_dir, texture_fn)\n",
    "    loss_model = StyleModel(input_texture_path)\n",
    "  elif loss_type == \"inception\":\n",
    "    layer_name, ch = loss_params.split(':')\n",
    "    loss_model = Inception(layer_name, int(ch))\n",
    "  return loss_model\n",
    "\n",
    "class TextureSynthTrainer:\n",
    "  def __init__(self, loss_model=None):\n",
    "    self.experiment_log_dir = \"%s/%s\"%(cfg.logdir, cfg.experiment_name)\n",
    "    self.writer = tf.summary.create_file_writer(self.experiment_log_dir)\n",
    "\n",
    "    if loss_model is None:\n",
    "      loss_model = create_loss_model()\n",
    "    self.loss_model = loss_model\n",
    "\n",
    "    self.ca = CAModel()\n",
    "    if tcfg.ancestor_npy:\n",
    "      self.ancestor_ca = CAModel()\n",
    "      ancestor_fn = \"%s/%s\" % (tcfg.ancestor_dir, tcfg.ancestor_npy)\n",
    "      self.ancestor_ca.load_params(ancestor_fn)\n",
    "      self.ca.load_params(ancestor_fn)\n",
    "      logging.info(\"loaded pre-trained model %s\" % tcfg.ancestor_npy)\n",
    "    self.loss_log = []\n",
    "    self.pool = SamplePool(x=self.seed_fn(tcfg.pool_size))\n",
    "    lr_sched = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "      [1000], [tcfg.lr, tcfg.lr*0.1])\n",
    "    self.trainer = tf.keras.optimizers.Adam(lr_sched)\n",
    "\n",
    "  def visualize_batch_tf(self, x0, x, step_num):\n",
    "    vis0 = np.hstack(to_rgb(x0))\n",
    "    vis1 = np.hstack(to_rgb(x))\n",
    "    vis = np.vstack([vis0, vis1])\n",
    "    tf.summary.image(\"batch_vis\", vis[None, ...])\n",
    "\n",
    "  def train(self):\n",
    "    with self.writer.as_default():\n",
    "      for _ in range(tcfg.train_steps+1):\n",
    "        step_num = len(self.loss_log)\n",
    "        step = self.train_step()\n",
    "        if step_num%50 == 0 or step_num == tcfg.train_steps:\n",
    "          self.visualize_batch_tf(step.x0, step.batch.x, step_num)\n",
    "          self.ca.save_params(\"%s/%s.npy\" % (cfg.logdir, cfg.experiment_name))\n",
    "        logging.info('step: %d, log10(loss): %s, loss: %s'%(len(self.loss_log), np.log10(step.loss), step.loss.numpy()))\n",
    "      self.save_video(\"%s/%s.mp4\" % (cfg.logdir, cfg.experiment_name), self.ca.embody)\n",
    "\n",
    "  def train_step(self):\n",
    "    step_num = len(self.loss_log)\n",
    "    tf.summary.experimental.set_step(step_num)\n",
    "    batch = self.pool.sample(tcfg.batch_size)\n",
    "    x0 = batch.x.copy()\n",
    "    if step_num%2==0:\n",
    "      x0[:1] = self.seed_fn(1)\n",
    "    batch.x[:], loss = self._train_step(x0)\n",
    "    batch.commit()\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    self.loss_log.append(loss.numpy())\n",
    "    return Bunch(batch=batch, x0=x0, loss=loss, step_num=step_num)\n",
    "\n",
    "  @tf.function\n",
    "  def _train_step(self, x):\n",
    "    iter_n = tf.random.uniform([], tcfg.rollout_len_min, tcfg.rollout_len_max, tf.int32)\n",
    "    with tf.GradientTape(persistent=False) as g:\n",
    "      f = self.ca.embody()\n",
    "      for i in tf.range(iter_n):\n",
    "        x = f(x)\n",
    "      loss = self.loss_model(to_rgb(x))\n",
    "    grads = g.gradient(loss, self.ca.params)\n",
    "    grads = [g/(tf.norm(g)+1e-8) for g in grads]\n",
    "    self.trainer.apply_gradients(zip(grads, self.ca.params))\n",
    "    return x, loss\n",
    "\n",
    "  def seed_fn(self, n):\n",
    "    states = np.zeros([n, tcfg.img_size, tcfg.img_size, tcfg.channel_n], np.float32)\n",
    "    return states\n",
    "\n",
    "  def save_video(self, path, f):\n",
    "    state = self.seed_fn(1)\n",
    "    f = self.ca.embody()\n",
    "    if tcfg.ancestor_npy:\n",
    "      state_ancestor = self.seed_fn(1)\n",
    "      f_ancestor = self.ancestor_ca.embody()\n",
    "    with VideoWriter(path, 60.0) as vid:\n",
    "      for i in range(tcfg.viz_rollout_len):\n",
    "        # visualize the RGB + hidden states.\n",
    "        if tcfg.hidden_viz_group:\n",
    "          padding_channel_len = (3 - state[0].shape[2] % 3) % 3\n",
    "          splitframe = np.split(np.pad(state[0], ((0,0), (0,0), (0,padding_channel_len)), mode='constant'), (state[0].shape[2] + padding_channel_len)/3, 2)\n",
    "        else:\n",
    "          hidden = np.transpose(np.repeat(state[0][..., 3:, None], 3, -1), (2, 0, 1, 3))\n",
    "          splitframe = np.concatenate([state[0][None, ..., :3], hidden], 0)\n",
    "        frame = to_rgb(tile2d(splitframe))\n",
    "        vid.add(frame)\n",
    "        if tcfg.ancestor_npy:\n",
    "          c_state = f(state, fire_rate=0.5)\n",
    "          a_state = f_ancestor(state, fire_rate=0.5)\n",
    "          progress = max(1.25*(i/tcfg.viz_rollout_len) - 0.25, 0.0)\n",
    "          state = (1-progress)*c_state + progress*a_state\n",
    "        else:\n",
    "          state = f(state, fire_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IQm90ZpzI4Jq"
   },
   "outputs": [],
   "source": [
    "def export_models_to_js(models, fixed_filter_n=4):\n",
    "  '''Exoprt numpy models in a form that ca.js can read.'''\n",
    "  model_names = list(models.keys())\n",
    "  models_js = {'model_names':model_names, 'layers': []}\n",
    "  params = models.values()\n",
    "  quant_scale_zero = [(2.0, 0.0), (4.0, 127.0 / 255.0)]\n",
    "  for i, layer in enumerate(zip(*params)):\n",
    "    shape = layer[0].shape\n",
    "    layer = np.array(layer)  # shape: [n, h, w]\n",
    "    if i == 0:\n",
    "      # Replaced with np equiv. for time being so this works internally.\n",
    "      # layer[:,:-1] = rearrange(layer[:,:-1], 'n (h c) w -> n (c h) w', c=fixed_filter_n)\n",
    "      s = layer[:, :-1].shape\n",
    "      layer[:, :-1] = (layer[:, :-1]\n",
    "                       .reshape(s[0], -1, fixed_filter_n, s[2])\n",
    "                       .transpose(0, 2, 1, 3)\n",
    "                       .reshape(s))\n",
    "    #layer = rearrange(layer, 'n h (w c) -> h (n w) c', c=4)\n",
    "    # N.B. this 4 is not the fixed filter number, but a webgl implementation detail.\n",
    "    # Pad when number of channels is not a multiple of 4.\n",
    "    s = layer.shape\n",
    "    layer = np.pad(layer, ((0,0), (0,0), (0, (4 - s[2]) % 4)), mode='constant')\n",
    "    layer = layer.reshape(s[0], s[1], -1, 4)\n",
    "    n, ht, wt = layer.shape[:3]\n",
    "    w = 1\n",
    "    while w<n and w*wt < (n+w-1)//w*ht:\n",
    "      w += 1\n",
    "    layer = tile2d(layer, w)\n",
    "    layout = (w, (n+w-1)//w)\n",
    "\n",
    "    scale = 2.0*np.abs(layer).max()\n",
    "    layer = np.round(layer/scale*255.0+127.0)\n",
    "    layer = np.uint8(layer.clip(0, 255))\n",
    "\n",
    "    url = im2url(layer, 'png')\n",
    "    layer_js = {'scale': scale,\n",
    "                'data': url,\n",
    "                'shape':shape,\n",
    "                'quant_scale_zero': quant_scale_zero[i],\n",
    "                'layout': layout}\n",
    "    models_js['layers'].append(layer_js)\n",
    "  return models_js\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzSEGWS6LpAc"
   },
   "source": [
    "# texture NCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "CmEuPoSYv2JS",
    "outputId": "dee5d209-6e4f-4d19-b7b8-407d3893e3e6"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texture_name = 'tenere.jpg'\n",
    "img_f = io.BytesIO(open(texture_name, 'rb').read())\n",
    "img = imread(img_f, cfg.texture_ca.vgg_input_img_size, mode='RGB')\n",
    "clear_output()\n",
    "imshow(img)\n",
    "imwrite('_target.png', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VobCdZDdTTKk",
    "outputId": "5d13e014-5908-4647-a4b9-52c27d5e0e9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/tsggdjzx2yb0mmhxy97qf45m0000gn/T/ipykernel_2848/1489709732.py:21: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  img.thumbnail((max_size, max_size), PIL.Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "loss_model = StyleModel('_target.png')\n",
    "out_fn = os.path.join(texture_name + '.npy')\n",
    "trainer = TextureSynthTrainer(loss_model=loss_model)\n",
    "r = trainer.train_step()\n",
    "# trainer.ca.save_params(out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGElYEyne9dN",
    "outputId": "abad734f-267b-40aa-e7d1-ccde578ef730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "params = np.load(out_fn, allow_pickle=True)\n",
    "for i, p in enumerate(trainer.ca.params):\n",
    "    print(np.all(np.isclose(p,params[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yo6wZYfKMbPQ"
   },
   "outputs": [],
   "source": [
    "models = {name:np.load(name, allow_pickle=True) for name in glob.glob('*.npy')}\n",
    "js_models = export_models_to_js(models)\n",
    "with open('models.json', 'w+') as f:\n",
    "    json.dump(js_models, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Antd-pdpBec1"
   ],
   "name": "texture_nca_tf2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05001ef30dab48ec88bcdbd5bbfdcdbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ab43228f33e49debc0d3344fce885d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "39e1ee8e006a4f4ca290acecac4f5e10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64b5a6ed1bd5499e92e1a302661c3734": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "673364ff02914c00b79512778210d710": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_821a4c426c3e43c8b6484c4e56d4983a",
       "IPY_MODEL_dc3d442f0b2e4c1382b27219441d21f7",
       "IPY_MODEL_9f0371cf79f34b79b2563ea3466e9825"
      ],
      "layout": "IPY_MODEL_05001ef30dab48ec88bcdbd5bbfdcdbb"
     }
    },
    "7342d6cbbef64999afe5326e1b7186c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c48e33755834744a34813965983ff45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "821a4c426c3e43c8b6484c4e56d4983a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_837c2d337e2149e3832cb9c4cbfbf1a3",
      "placeholder": "​",
      "style": "IPY_MODEL_7c48e33755834744a34813965983ff45",
      "value": " 99%"
     }
    },
    "837c2d337e2149e3832cb9c4cbfbf1a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f0371cf79f34b79b2563ea3466e9825": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64b5a6ed1bd5499e92e1a302661c3734",
      "placeholder": "​",
      "style": "IPY_MODEL_39e1ee8e006a4f4ca290acecac4f5e10",
      "value": " 297/300 [00:06&lt;00:00, 49.95it/s]"
     }
    },
    "dc3d442f0b2e4c1382b27219441d21f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7342d6cbbef64999afe5326e1b7186c9",
      "max": 300,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ab43228f33e49debc0d3344fce885d9",
      "value": 300
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
